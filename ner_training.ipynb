{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dataframe,word_to_idx,tag_to_idx,max_length):\n",
    "        \n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "        self.data = dataframe\n",
    "\n",
    "        \n",
    "        grouped = self.data.groupby(\"id\")\n",
    "        \n",
    "        for _,grouped in grouped:\n",
    "            \n",
    "            sentence = [word_to_idx[word] for word in grouped['word'].tolist()]\n",
    "            labels_ = []\n",
    "            for tag in grouped['tag'].tolist():\n",
    "                labels_.append(tag_to_idx[tag])\n",
    "                \n",
    "            remaining_len = max_length - len(sentence)\n",
    "            # print(remaining_len,max_length,len(sentence))\n",
    "            for i in range(remaining_len):\n",
    "                sentence.append(word_to_idx[\"<E>\"])\n",
    "                labels_.append(tag_to_idx[\"O\"])\n",
    "            self.sentences.append(sentence)\n",
    "            # print(np.unique(self.sentences))\n",
    "            self.labels.append(labels_)\n",
    "        # print(np.unique([ len(sent) for sent in self.sentences]))\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        return (self.sentences[idx],self.labels[idx])\n",
    "    \n",
    "    def collate_fn(self,batch):\n",
    "        \n",
    "        sentences,labels = zip(*batch)\n",
    "        max_len = max(len(s) for s in sentences)\n",
    "        \n",
    "        padded_sentences = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for sentence,label in zip(sentences,labels):\n",
    "            \n",
    "            padded_sentences.append(sentence+[0]*(max_len-len(sentence)))\n",
    "            padded_labels.append(label +[0]*(max_len-len(label)))\n",
    "            \n",
    "        padded_sentences = torch.tensor(padded_sentences,dtype=torch.long)\n",
    "        padded_labels = torch.tensor(padded_labels,dtype=torch.long)\n",
    "        \n",
    "        return (padded_sentences,padded_labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_data_v1.csv\",index_col=None)\n",
    "unique_words = sorted(list(df['word'].unique()))\n",
    "\n",
    "word_to_idx = {word:i for i,word in enumerate(unique_words)}\n",
    "idx_to_word = {i:word for i,word in enumerate(unique_words)}\n",
    "\n",
    "# for appending <E> : extra\n",
    "word_to_idx[\"<E>\"]=len(word_to_idx)\n",
    "idx_to_word[len(word_to_idx)]=\"<E>\"\n",
    "\n",
    "group_counts = df.groupby('id').size()\n",
    "# print(group_counts)\n",
    "# max sentence length\n",
    "max_sentence_len = group_counts.max()\n",
    "print(max_sentence_len)\n",
    "\n",
    "size = df.shape[0]\n",
    "split = int(size*0.8)\n",
    "\n",
    "tag_to_idx = {\n",
    "    \n",
    "    \"O\":0,\n",
    "    \"B-ro\":1,\n",
    "    \"I-ro\":2,\n",
    "    \"B-cm\":3,\n",
    "    \"I-cm\":4,\n",
    "    \"B-sk\":5,\n",
    "    \"I-sk\":6\n",
    "}\n",
    "idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "\n",
    "train_dataset = NERDataset(df[:split],word_to_idx,tag_to_idx,max_sentence_len)\n",
    "test_dataset = NERDataset(df[split:],word_to_idx,tag_to_idx,max_sentence_len)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,32,shuffle=True,collate_fn=train_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset,32,shuffle=True,collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Title</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ai</td>\n",
       "      <td>B-ro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>researcher</td>\n",
       "      <td>I-ro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Company</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9491471</th>\n",
       "      <td>9491471</td>\n",
       "      <td>29999</td>\n",
       "      <td>big</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9491472</th>\n",
       "      <td>9491472</td>\n",
       "      <td>29999</td>\n",
       "      <td>?</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9491473</th>\n",
       "      <td>9491473</td>\n",
       "      <td>29999</td>\n",
       "      <td>Apply</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9491474</th>\n",
       "      <td>9491474</td>\n",
       "      <td>29999</td>\n",
       "      <td>today</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9491475</th>\n",
       "      <td>9491475</td>\n",
       "      <td>29999</td>\n",
       "      <td>!</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9491476 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0     id        word   tag\n",
       "0                 0      0       Title     O\n",
       "1                 1      0           :     O\n",
       "2                 2      0          ai  B-ro\n",
       "3                 3      0  researcher  I-ro\n",
       "4                 4      0     Company     O\n",
       "...             ...    ...         ...   ...\n",
       "9491471     9491471  29999         big     O\n",
       "9491472     9491472  29999           ?     O\n",
       "9491473     9491473  29999       Apply     O\n",
       "9491474     9491474  29999       today     O\n",
       "9491475     9491475  29999           !     O\n",
       "\n",
       "[9491476 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from TorchCRF import CRF\n",
    "\n",
    "\n",
    "class BILSTM_CRF(nn.Module):\n",
    "    def __init__(self,vocab_size,tagset_size,num_layers=2,embedding_dim=100,hidden_dim=128,verbose=False,device='cuda'):\n",
    "        super(BILSTM_CRF,self).__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tagset_size = tagset_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim//2,num_layers=1,bidirectional=True)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
    "        if device=='cuda':\n",
    "            flag=True\n",
    "        else:\n",
    "            flag=False\n",
    "            \n",
    "        self.crf = CRF(tagset_size,use_gpu=flag)\n",
    "        \n",
    "        \n",
    "    def forward(self,sentences,tags=None):\n",
    "        \n",
    "        embeds = self.embedding_layer(sentences)\n",
    "        if self.verbose:\n",
    "            print(f\"embedding layer shape: {embeds.shape}\")\n",
    "            \n",
    "        mask = sentences != 0  # Assuming padding index is zero\n",
    "        \n",
    "        batch_size = sentences.size(1)\n",
    "        h_0 = torch.zeros(self.num_layers,batch_size,self.hidden_dim//2).to(sentences.device)\n",
    "        c_0 = torch.zeros(self.num_layers,batch_size,self.hidden_dim//2).to(sentences.device)\n",
    "        \n",
    "        lstm_out,_ = self.lstm(embeds,(h_0,c_0))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"lstm layer shape: {lstm_out.shape}\")\n",
    "            \n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        if self.verbose:\n",
    "            print(f\"emissions layer shape: {emissions.shape}\")\n",
    "        \n",
    "        if tags is not None:\n",
    "\n",
    "            loss = -self.crf(emissions,tags,mask=mask.byte())\n",
    "            tag_seq = self.crf.viterbi_decode(emissions,mask=mask.byte())\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"crf emission layer shape: {loss.shape}\")\n",
    "            return (loss,tag_seq)\n",
    "        else:\n",
    "            tag_seq = self.crf.viterbi_decode(emissions,mask=mask.byte())\n",
    "            if self.verbose:\n",
    "                print(f\"crf emission layer shape: {len(tag_seq)}\")\n",
    "            return tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_idx)\n",
    "taget_size = len(tag_to_idx)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = BILSTM_CRF(vocab_size,taget_size,num_layers,embedding_dim,hidden_dim)\n",
    "\n",
    "# sentence,label = next(iter(train_dataloader))\n",
    "# # sentence\n",
    "# res = model(sentence[6].unsqueeze(0))\n",
    "\n",
    "# decoded_sentence = \"\"\n",
    "# decoded_labels = \"\"\n",
    "\n",
    "# for idx in sentence[6].numpy():\n",
    "#     decoded_sentence+=idx_to_word[idx]+\" \"\n",
    "    \n",
    "# for idx in res[0]:\n",
    "#     decoded_labels+=idx_to_tag[idx]+\" \"\n",
    "    \n",
    "# print(decoded_sentence)\n",
    "# print(decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(model,train_dataloader,optimizer,device='cuda'):\n",
    "    model.train()\n",
    "    train_loss=0\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    total_batches = len(train_dataloader)\n",
    "    for batch,(sentence,labels) in enumerate(train_dataloader):\n",
    "        # print(sentence.shape)\n",
    "        sentence = sentence.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss,output = model(sentence,labels)\n",
    "        predictions.append(output)\n",
    "        ground_truth.append(labels.tolist())\n",
    "#         train_loss+=loss.mean().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(batch+1)%50 == 0:\n",
    "            print(f\"  Batch {batch+1} / {total_batches}; Loss: {loss.item():.4f}\")\n",
    "#     print(predictions[0],ground_truth)\n",
    "    train_acc = compute_accuracy(predictions,ground_truth)\n",
    "    \n",
    "    return train_acc,train_loss\n",
    "\n",
    "# def compute_accuracy(predictions, ground_truth):\n",
    "#     predictions = np.array(predictions[0])\n",
    "#     ground_truth = np.array(ground_truth[0])\n",
    "\n",
    "#     correct = predictions == ground_truth\n",
    "#     average_accuracy = np.mean(correct)\n",
    "\n",
    "#     return average_accuracy\n",
    "\n",
    "\n",
    "                \n",
    "# def compute_accuracy(predictions, ground_truth):\n",
    "#     correct = 0\n",
    "#     total_acc = 0\n",
    "#     total = len(ground_truth[0])\n",
    "#     print(ground_truth[0])\n",
    "#     for preds, targets in zip(predictions, ground_truth):\n",
    "# #         print(preds,targets)\n",
    "#         temp_total= len(targets[0])\n",
    "#         temp_acc = 0\n",
    "#         correct=0\n",
    "#         for pred, target in zip(preds[0], targets[0]):\n",
    "#             if pred == target:\n",
    "#                 correct += 1\n",
    "        \n",
    "#         total_acc += (correct/temp_total)\n",
    "# #         print(total_acc,correct,temp_total)\n",
    "# #     print(total)\n",
    "#     return total_acc / total\n",
    "\n",
    "def compute_accuracy(predictions, ground_truth):\n",
    "    # Ensuring all elements are numpy arrays of consistent shape\n",
    "    accuracies = []\n",
    "    for pred, ground in zip(predictions, ground_truth):\n",
    "        pred = np.array(pred[0])\n",
    "        ground = np.array(ground[0])\n",
    "        if pred.shape == ground.shape:\n",
    "            # Perform element-wise comparison and calculate accuracy\n",
    "            correct = np.sum(pred == ground)\n",
    "            accuracy = correct / len(pred)\n",
    "            accuracies.append(accuracy)\n",
    "        else:\n",
    "            # print(pred.shape,ground.shape)\n",
    "            # print(\"error\")\n",
    "            pass\n",
    "\n",
    "    average_accuracy = np.mean(accuracies) if accuracies else 0\n",
    "    return average_accuracy\n",
    "                \n",
    "                \n",
    "def test_step(model,test_dataloader,optimizer,device='cuda'):\n",
    "    \n",
    "    model.eval()\n",
    "    total_batches = len(test_dataloader)\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence,labels in test_dataloader: #for each batch\n",
    "\n",
    "            sentence = sentence.to(device)\n",
    "            ground_truth.append(labels.tolist())\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            output = model(sentence)\n",
    "            \n",
    "            predictions.append(output)\n",
    "    \n",
    "    acc = compute_accuracy(predictions,ground_truth)\n",
    "            \n",
    "#     print(f\"   Test Accuracy: {acc*100:.2f}\")\n",
    "    return acc\n",
    "    \n",
    "    \n",
    "def train(model,train_dataloader,test_dataloader,optimizer,epochs,device='cuda'):\n",
    "    \n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} :\")\n",
    "        train_acc,train_loss = train_step(model,train_dataloader,optimizer,device=device)\n",
    "        test_acc = test_step(model,test_dataloader,optimizer,device=device)\n",
    "        \n",
    "        results[\"train_acc\"] = train_acc\n",
    "        results[\"train_loss\"]=train_loss\n",
    "        results[\"test_acc\"] = test_acc\n",
    "        #| Train Loss: {train_loss*100:.2f}%\n",
    "        print(f\"Epoch: {epoch+1} |  Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}%\")\n",
    "        print(\"---------------------------------------------------------------------------------------\")\n",
    "        \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch 1 :\n",
      "  Batch 50 / 752; Loss: 157.5845\n",
      "  Batch 100 / 752; Loss: 94.4996\n",
      "  Batch 150 / 752; Loss: 58.9334\n",
      "  Batch 200 / 752; Loss: 48.0856\n",
      "  Batch 250 / 752; Loss: 30.4945\n",
      "  Batch 300 / 752; Loss: 21.0713\n",
      "  Batch 350 / 752; Loss: 16.2867\n",
      "  Batch 400 / 752; Loss: 19.9682\n",
      "  Batch 450 / 752; Loss: 13.6588\n",
      "  Batch 500 / 752; Loss: 14.0016\n",
      "  Batch 550 / 752; Loss: 14.2538\n",
      "  Batch 600 / 752; Loss: 13.5047\n",
      "  Batch 650 / 752; Loss: 12.0423\n",
      "  Batch 700 / 752; Loss: 10.8470\n",
      "  Batch 750 / 752; Loss: 11.2211\n",
      "Epoch: 1 |  Train Acc: 97.72% | Test Acc: 99.25%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 2 :\n",
      "  Batch 50 / 752; Loss: 11.2557\n",
      "  Batch 100 / 752; Loss: 8.6513\n",
      "  Batch 150 / 752; Loss: 10.2185\n",
      "  Batch 200 / 752; Loss: 10.0833\n",
      "  Batch 250 / 752; Loss: 8.8912\n",
      "  Batch 300 / 752; Loss: 8.8662\n",
      "  Batch 350 / 752; Loss: 7.3540\n",
      "  Batch 400 / 752; Loss: 11.4080\n",
      "  Batch 450 / 752; Loss: 9.8568\n",
      "  Batch 500 / 752; Loss: 7.6368\n",
      "  Batch 550 / 752; Loss: 7.4535\n",
      "  Batch 600 / 752; Loss: 8.9230\n",
      "  Batch 650 / 752; Loss: 8.1019\n",
      "  Batch 700 / 752; Loss: 8.5152\n",
      "  Batch 750 / 752; Loss: 7.4872\n",
      "Epoch: 2 |  Train Acc: 99.41% | Test Acc: 99.51%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 3 :\n",
      "  Batch 50 / 752; Loss: 6.4443\n",
      "  Batch 100 / 752; Loss: 6.2589\n",
      "  Batch 150 / 752; Loss: 5.8753\n",
      "  Batch 200 / 752; Loss: 6.4269\n",
      "  Batch 250 / 752; Loss: 5.3657\n",
      "  Batch 300 / 752; Loss: 4.9413\n",
      "  Batch 350 / 752; Loss: 5.9654\n",
      "  Batch 400 / 752; Loss: 4.7589\n",
      "  Batch 450 / 752; Loss: 5.6568\n",
      "  Batch 500 / 752; Loss: 5.8196\n",
      "  Batch 550 / 752; Loss: 4.1586\n",
      "  Batch 600 / 752; Loss: 4.5515\n",
      "  Batch 650 / 752; Loss: 4.7941\n",
      "  Batch 700 / 752; Loss: 5.3494\n",
      "  Batch 750 / 752; Loss: 4.7677\n",
      "Epoch: 3 |  Train Acc: 99.69% | Test Acc: 99.82%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 4 :\n",
      "  Batch 50 / 752; Loss: 3.5921\n",
      "  Batch 100 / 752; Loss: 5.4823\n",
      "  Batch 150 / 752; Loss: 4.0404\n",
      "  Batch 200 / 752; Loss: 5.1337\n",
      "  Batch 250 / 752; Loss: 3.6035\n",
      "  Batch 300 / 752; Loss: 4.0849\n",
      "  Batch 350 / 752; Loss: 4.2882\n",
      "  Batch 400 / 752; Loss: 4.2648\n",
      "  Batch 450 / 752; Loss: 3.2686\n",
      "  Batch 500 / 752; Loss: 3.7826\n",
      "  Batch 550 / 752; Loss: 3.2443\n",
      "  Batch 600 / 752; Loss: 4.3330\n",
      "  Batch 650 / 752; Loss: 3.2325\n",
      "  Batch 700 / 752; Loss: 3.2234\n",
      "  Batch 750 / 752; Loss: 3.2697\n",
      "Epoch: 4 |  Train Acc: 99.81% | Test Acc: 99.86%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 5 :\n",
      "  Batch 50 / 752; Loss: 3.5054\n",
      "  Batch 100 / 752; Loss: 3.2775\n",
      "  Batch 150 / 752; Loss: 2.5094\n",
      "  Batch 200 / 752; Loss: 2.7811\n",
      "  Batch 250 / 752; Loss: 3.0583\n",
      "  Batch 300 / 752; Loss: 2.0040\n",
      "  Batch 350 / 752; Loss: 3.1302\n",
      "  Batch 400 / 752; Loss: 2.6832\n",
      "  Batch 450 / 752; Loss: 2.4647\n",
      "  Batch 500 / 752; Loss: 1.7491\n",
      "  Batch 550 / 752; Loss: 2.1100\n",
      "  Batch 600 / 752; Loss: 3.4122\n",
      "  Batch 650 / 752; Loss: 1.5535\n",
      "  Batch 700 / 752; Loss: 2.2792\n",
      "  Batch 750 / 752; Loss: 1.7347\n",
      "Epoch: 5 |  Train Acc: 99.87% | Test Acc: 99.93%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 6 :\n",
      "  Batch 50 / 752; Loss: 2.4165\n",
      "  Batch 100 / 752; Loss: 1.5837\n",
      "  Batch 150 / 752; Loss: 1.6683\n",
      "  Batch 200 / 752; Loss: 1.8995\n",
      "  Batch 250 / 752; Loss: 1.6515\n",
      "  Batch 300 / 752; Loss: 1.5382\n",
      "  Batch 350 / 752; Loss: 1.3773\n",
      "  Batch 400 / 752; Loss: 1.5017\n",
      "  Batch 450 / 752; Loss: 1.1848\n",
      "  Batch 500 / 752; Loss: 1.3291\n",
      "  Batch 550 / 752; Loss: 2.1217\n",
      "  Batch 600 / 752; Loss: 2.0738\n",
      "  Batch 650 / 752; Loss: 1.7814\n",
      "  Batch 700 / 752; Loss: 1.9865\n",
      "  Batch 750 / 752; Loss: 1.3612\n",
      "Epoch: 6 |  Train Acc: 99.92% | Test Acc: 99.86%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 7 :\n",
      "  Batch 50 / 752; Loss: 1.1234\n",
      "  Batch 100 / 752; Loss: 1.3809\n",
      "  Batch 150 / 752; Loss: 1.3814\n",
      "  Batch 200 / 752; Loss: 1.3635\n",
      "  Batch 250 / 752; Loss: 1.2097\n",
      "  Batch 300 / 752; Loss: 1.2930\n",
      "  Batch 350 / 752; Loss: 2.2117\n",
      "  Batch 400 / 752; Loss: 1.0533\n",
      "  Batch 450 / 752; Loss: 1.3247\n",
      "  Batch 500 / 752; Loss: 1.6529\n",
      "  Batch 550 / 752; Loss: 1.0397\n",
      "  Batch 600 / 752; Loss: 0.8103\n",
      "  Batch 650 / 752; Loss: 1.4882\n",
      "  Batch 700 / 752; Loss: 0.9429\n",
      "  Batch 750 / 752; Loss: 0.8559\n",
      "Epoch: 7 |  Train Acc: 99.92% | Test Acc: 99.92%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 8 :\n",
      "  Batch 50 / 752; Loss: 0.7517\n",
      "  Batch 100 / 752; Loss: 0.8675\n",
      "  Batch 150 / 752; Loss: 0.7244\n",
      "  Batch 200 / 752; Loss: 0.5528\n",
      "  Batch 250 / 752; Loss: 1.3595\n",
      "  Batch 300 / 752; Loss: 0.6826\n",
      "  Batch 350 / 752; Loss: 0.7765\n",
      "  Batch 400 / 752; Loss: 0.9277\n",
      "  Batch 450 / 752; Loss: 0.3389\n",
      "  Batch 500 / 752; Loss: 1.0032\n",
      "  Batch 550 / 752; Loss: 0.6770\n",
      "  Batch 600 / 752; Loss: 0.6813\n",
      "  Batch 650 / 752; Loss: 0.5237\n",
      "  Batch 700 / 752; Loss: 0.6245\n",
      "  Batch 750 / 752; Loss: 1.4718\n",
      "Epoch: 8 |  Train Acc: 99.94% | Test Acc: 99.94%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 9 :\n",
      "  Batch 50 / 752; Loss: 0.6731\n",
      "  Batch 100 / 752; Loss: 0.8295\n",
      "  Batch 150 / 752; Loss: 0.8086\n",
      "  Batch 200 / 752; Loss: 0.7151\n",
      "  Batch 250 / 752; Loss: 1.1140\n",
      "  Batch 300 / 752; Loss: 0.7482\n",
      "  Batch 350 / 752; Loss: 1.4348\n",
      "  Batch 400 / 752; Loss: 0.9782\n",
      "  Batch 450 / 752; Loss: 2.0302\n",
      "  Batch 500 / 752; Loss: 1.7771\n",
      "  Batch 550 / 752; Loss: 1.2442\n",
      "  Batch 600 / 752; Loss: 1.0025\n",
      "  Batch 650 / 752; Loss: 1.2679\n",
      "  Batch 700 / 752; Loss: 1.1027\n",
      "  Batch 750 / 752; Loss: 0.8778\n",
      "Epoch: 9 |  Train Acc: 99.90% | Test Acc: 99.88%\n",
      "---------------------------------------------------------------------------------------\n",
      "Epoch 10 :\n",
      "  Batch 50 / 752; Loss: 2.1529\n",
      "  Batch 100 / 752; Loss: 1.8913\n",
      "  Batch 150 / 752; Loss: 4.8492\n",
      "  Batch 200 / 752; Loss: 11.7234\n",
      "  Batch 250 / 752; Loss: 11.7952\n",
      "  Batch 300 / 752; Loss: 10.1699\n",
      "  Batch 350 / 752; Loss: 9.7346\n",
      "  Batch 400 / 752; Loss: 9.4440\n",
      "  Batch 450 / 752; Loss: 6.6941\n",
      "  Batch 500 / 752; Loss: 10.6653\n",
      "  Batch 550 / 752; Loss: 10.5889\n",
      "  Batch 600 / 752; Loss: 17.4685\n",
      "  Batch 650 / 752; Loss: 11.1772\n",
      "  Batch 700 / 752; Loss: 12.3198\n",
      "  Batch 750 / 752; Loss: 11.3362\n",
      "Epoch: 10 |  Train Acc: 99.70% | Test Acc: 99.78%\n",
      "---------------------------------------------------------------------------------------\n",
      "Total training time: 6946.043 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "vocab_size = len(word_to_idx)\n",
    "taget_size = len(tag_to_idx)\n",
    "embedding_dim = 250\n",
    "hidden_dim = 164\n",
    "num_layers = 2\n",
    "epochs = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "model = BILSTM_CRF(vocab_size,taget_size,num_layers,embedding_dim,hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "model_results = train(model,train_dataloader,test_dataloader,optimizer,epochs,device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"model_0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 553])\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "tesla B-cm B-cm\n",
      "tesla B-cm B-cm\n",
      "tesla B-cm B-cm\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "dynamodb B-sk B-sk\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "Firebase B-sk B-sk\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "hypertable B-sk B-sk\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "multivariate B-sk B-sk\n",
      "testing I-sk I-sk\n",
      "Simulink B-sk B-sk\n",
      "groovy B-sk B-sk\n",
      "Microsoft B-sk B-sk\n",
      "Dynamics I-sk I-sk\n",
      "365 I-sk I-sk\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "support B-ro B-ro\n",
      "engineer I-ro I-ro\n",
      "tesla B-cm B-cm\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_idx)\n",
    "taget_size = len(tag_to_idx)\n",
    "embedding_dim = 250\n",
    "hidden_dim = 164\n",
    "num_layers = 2\n",
    "device = 'cuda'\n",
    "\n",
    "model = BILSTM_CRF(vocab_size,taget_size,num_layers,embedding_dim,hidden_dim).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_0.pt\"))\n",
    "\n",
    "sentence,labelss = next(iter(test_dataloader))\n",
    "# sentence\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(sentence[6].unsqueeze(0).shape)\n",
    "    res = model(sentence[6].unsqueeze(0).to('cuda'))\n",
    "\n",
    "    decoded_sentence = []\n",
    "    decoded_labels = []\n",
    "    original_labels = []\n",
    "    sent = sentence[6].numpy()\n",
    "    labelss = labelss[6].numpy()\n",
    "    \n",
    "    for idx in range(len(sent)):\n",
    "        try:\n",
    "            decoded_sentence.append(idx_to_word[sent[idx]])\n",
    "            original_labels.append(idx_to_tag[labelss[idx]])\n",
    "        except:\n",
    "            pass\n",
    "        # print(decoded_sentence)\n",
    "\n",
    "    for idx in res[0]:\n",
    "        decoded_labels.append(idx_to_tag[idx])\n",
    "        \n",
    "    for word,label,org_label in zip(decoded_sentence,decoded_labels,original_labels):\n",
    "        \n",
    "        if label!='O':\n",
    "            \n",
    "            print(word,label,org_label)\n",
    "\n",
    "    # print(decoded_sentence)\n",
    "    # print(decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_labels(model,sentence):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_words = []\n",
    "        \n",
    "        for word in sentence.split():\n",
    "            \n",
    "            try:\n",
    "                input_words.append(word_to_idx[word])\n",
    "            except:\n",
    "                # print(word)\n",
    "                pass\n",
    "            \n",
    "        # print(input_words)\n",
    "        remaining_len = 473 - len(sentence)\n",
    "        for i in range(remaining_len):\n",
    "            input_words.append(word_to_idx[\"<E>\"])\n",
    "        org_sentence = []\n",
    "        for idx in input_words:\n",
    "            try:\n",
    "                org_sentence.append(idx_to_word[idx])\n",
    "            except:\n",
    "                pass\n",
    "        input_words = torch.tensor(input_words)\n",
    "        # print(input_words.unsqueeze(0).shape)\n",
    "        res = model(input_words.unsqueeze(0).to('cuda'))\n",
    "        decoded_labels = []\n",
    "        for idx in res[0]:\n",
    "            decoded_labels.append(idx_to_tag[idx])\n",
    "        \n",
    "        for word,label in zip(org_sentence,decoded_labels):\n",
    "            if label!='O':\n",
    "                print(word,label)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyber B-ro\n",
      "security I-ro\n",
      "service B-cm\n",
      "cloud B-ro\n",
      "model B-sk\n",
      "public B-sk\n",
      "secure B-sk\n",
      "across B-sk\n",
      "infrastructure I-sk\n",
      "python B-sk\n",
      "the B-sk\n",
      "programming I-sk\n",
      "language I-sk\n",
      "TensorFlow B-sk\n",
      "Keras I-sk\n",
      "machine B-sk\n",
      "learning I-sk\n",
      "cloud B-ro\n",
      "machine B-sk\n",
      "learning I-sk\n",
      "service I-sk\n",
      "Google B-sk\n",
      "Cloud I-sk\n",
      "cloud B-ro\n",
      "control B-sk\n"
     ]
    }
   ],
   "source": [
    "sentence = '''\n",
    "About the job\n",
    "Are you looking to elevate your cyber career? Your technical skills? Your opportunity for growth? Deloitte's Government and Public Services Cyber Practice (GPS Cyber Practice) is the place for you! Our GPS Cyber Practice helps organizations create a cyber minded culture and become stronger, faster, and more innovative. You will become part of a team that advises, implements, and manages solutions across five verticals: Strategy, Defense and Response; Identity; Infrastructure; Data; and Application Security. Our dynamic team offers opportunities to work with cutting-edge cyber security tools and grow both vertically and horizontally at an accelerated rate. Join our cyber team and elevate your career.\n",
    "\n",
    "Work you'll do\n",
    "\n",
    "Develop AI-based tools that drive efficiencies in service delivery.\n",
    "Collaborate with a cloud development team for model integration.\n",
    "\n",
    "The team\n",
    "\n",
    "Deloitte's Government and Public Services (GPS) practice - our people, ideas, technology, and outcomes-is designed for impact. Serving federal, state, & local government clients as well as public higher education institutions, our team of more than 15,000 professionals brings fresh perspective to help clients anticipate disruption, reimagine the possible, and fulfill their mission promise\n",
    "\n",
    "At Deloitte, we believe cyber is about starting things-not stopping them-and enabling the freedom to create a more secure future. Cyber Infrastructure is focused on rethinking how security is integrated across modernized infrastructure as cyber threats become more complex. If you're seeking a career implementing, architecting, and-in select cases-handling next generation controls to manage security risks and exposure, then the Cyber Infrastructure team at Deloitte is for you.\n",
    "\n",
    "Qualifications\n",
    "\n",
    "Required:\n",
    "\n",
    "Bachelor's degree required.\n",
    "1 year plus experience in python development.\n",
    "1 year plus experience in Machine Learning experience, irrespective of the programming language\n",
    "1 year plus experience or college course level learning with Python Machine Learning libraries, especially pytorch. TensorFlow and/or Keras experience are also relevant.\n",
    "Must be legally authorized to work in the United States without the need for employer sponsorship, now or at any time in the future.\n",
    "Must be able to obtain and maintain the required clearance for this role.\n",
    "Ability to travel 0-15%, on average, based on the work you do and the clients and industries/sectors you serve.\n",
    "\n",
    "Preferred Requirements\n",
    "\n",
    "6 months plus of mathematics that power machine learning models, including Statistics, Probability, Linear Algebra, Calculus\n",
    "6 months plus experience developing in a cloud native machine learning service such as Google Cloud Vertex AI or AWS SageMaker\n",
    "6 months plus experience deploying AI/ML models in docker, cloud environments, or API.\n",
    "6 months plus experience using GitHub for version control and source code management.\n",
    "Prior professional services or federal consulting experience\n",
    "\n",
    "Information for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html\n",
    "'''\n",
    "\n",
    "predict_labels(model,sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplab_env",
   "language": "python",
   "name": "deeplab_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
